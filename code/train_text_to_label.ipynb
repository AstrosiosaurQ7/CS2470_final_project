{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from data import get_data\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label         0  \\\n",
      "0  One of the best silly horror movies of recent ...      1 -0.267310   \n",
      "1  Jason Patric and Ray Liotta make for one splen...      4  0.080934   \n",
      "2               This is more a case of `Sacre bleu!'      2  0.432747   \n",
      "3  Presents a good case while failing to provide ...      3 -0.023501   \n",
      "4  Beautifully crafted, engaging filmmaking that ...      4 -0.178820   \n",
      "\n",
      "          1         2         3         4         5         6         7  ...  \\\n",
      "0 -0.488523  0.037029  0.158269 -0.156495 -0.319468 -0.075835  0.832508  ...   \n",
      "1 -0.041667 -0.094109 -0.178179 -0.209157 -0.398069  0.276074  0.265166  ...   \n",
      "2  0.268934 -0.095764 -0.432659 -0.464035 -0.604674  0.697554  0.361407  ...   \n",
      "3  0.231426 -0.376311 -0.125749 -0.120390 -0.270657  0.208131  0.349706  ...   \n",
      "4 -0.415945  0.350926 -0.195871  0.096942 -0.267285  0.101068  0.409448  ...   \n",
      "\n",
      "        758       759       760       761       762       763       764  \\\n",
      "0 -0.139409 -0.382803 -0.014293  0.162741 -0.119586 -0.145988 -0.284225   \n",
      "1  0.019882 -0.279510  0.487214 -0.063319 -0.317990  0.307410 -0.138847   \n",
      "2 -0.019285  0.018970  0.098445 -0.216882  0.180968  0.065956 -0.143221   \n",
      "3 -0.175634 -0.053907 -0.203109 -0.171543 -0.175331 -0.187880  0.313981   \n",
      "4  0.014798 -0.497767 -0.254840 -0.153944 -0.391385 -0.175468  0.061827   \n",
      "\n",
      "        765       766       767  \n",
      "0 -0.074995  0.574106  0.162554  \n",
      "1 -0.294239  0.632328  0.194201  \n",
      "2  0.070759  0.683847  0.956834  \n",
      "3 -0.208403  0.154600  0.266118  \n",
      "4 -0.352147  0.469257  0.272196  \n",
      "\n",
      "[5 rows x 770 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'sentence': [\n",
    "        \"One of the best silly horror movies of recent memory, with some real shocks in store for unwary viewers.\",\n",
    "        \"Jason Patric and Ray Liotta make for one splendidly cast pair.\",\n",
    "        \"This is more a case of `Sacre bleu!'\",\n",
    "        \"Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.\",\n",
    "        \"Beautifully crafted, engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic, twisty yarn that will keep them guessing.\",\n",
    "        \"Bread, My Sweet has so many flaws it would be easy for critics to shred it.\",\n",
    "        \"Ultimately feels empty and unsatisfying, like swallowing a Communion wafer without the wine.\",\n",
    "        \"Exudes the fizz of a Busby Berkeley musical and the visceral excitement of a sports extravaganza.\"\n",
    "    ],\n",
    "    'label': [1, 4, 2, 3, 4, 2, 3, 1]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization and encoding\n",
    "tokens = tokenizer(df['sentence'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state[:,0,:].numpy()  # Taking the output of the first token (CLS token)\n",
    "\n",
    "# Append embeddings to DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "processed_df = pd.concat([df, embeddings_df], axis=1)\n",
    "\n",
    "# Output the processed data\n",
    "print(processed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': ['1', '2', '4', '5', '6', '7', '9', '10', '11', '12', '14', '15', '16', '19', '20', '21', '23', '24', '25', '26', '28', '29', '30', '32', '33', '35', '36', '37', '38', '39', '40', '41', '42', '43', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '56', '57', '58', '59', '60', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '75', '76', '77', '78', '79', '80', '81', '82', '83', '85', '86', '87', '88', '89', '90', '92', '93', '95', '96', '97', '98', '99', '100'], 'sentence': ['One of the best silly horror movies of recent memory, with some real shocks in store for unwary viewers.', 'Jason Patric and Ray Liotta make for one splendidly cast pair.', \"This is more a case of `Sacre bleu!'\", 'Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.', 'Beautifully crafted, engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic, twisty yarn that will keep them guessing.', 'Bread, My Sweet has so many flaws it would be easy for critics to shred it.', 'Ultimately feels emp11111ty and unsatisfying, like swallowing a Communion wafer without the wine.', 'Exudes the fizz of a Busby Berkeley musical and the visceral excitement of a sports extravaganza.', \"The film rehashes several old themes and is capped with pointless extremes -- it's insanely violent and very graphic.\", 'Ryan Gosling is, in a word, brilliant as the conflicted Daniel.', \"If Deuces Wild had been tweaked up a notch it would have become a camp adventure, one of those movies that's so bad it starts to become good.\", \"The film's stagecrafts are intimate and therefore bolder than the otherwise calculated artifice that defines and overwhelms the film's production design.\", 'Frida is certainly no disaster, but neither is it the Kahlo movie Frida fans have been looking for.', \"The movie's plot is almost entirely witless and inane, carrying every gag two or three times beyond its limit to sustain a laugh.\", 'For the most part, the ingredients are there.', \"But if you've paid a matinee price and bought a big tub of popcorn, there's guilty fun to be had here.\", \"The acting is stiff, the story lacks all trace of wit, the sets look like they were borrowed from Gilligan's Island -- and the CGI Scooby might well be the worst special-effects creation of the year.\", \"This version moves beyond the original's nostalgia for the communal film experiences of yesteryear to a deeper realization of cinema's inability to stand in for true, lived experience.\", \"There's no palpable chemistry between Lopez and male lead Ralph Fiennes, plus the script by Working Girl scribe Kevin Wade is workmanlike in the extreme.\", 'A first-class, thoroughly involving B movie that effectively combines two surefire, beloved genres -- the prison flick and the fight film.', 'Weiss and Speck never make a convincing case for the relevance of these two 20th-century footnotes.', 'Another Best of the Year selection.', \"It's all a rather shapeless good time...\", 'I love the robust middle of this picture.', \"That's not vintage Spielberg and that, finally, is minimally satisfying.\", \"Mostly Martha could have used a little trimming -- 10 or 15 minutes could be cut and no one would notice -- but it's a pleasurable trifle.\", \"For the rest of us, sitting through Dahmer's two hours amounts to little more than punishment.\", \"Featuring a dangerously seductive performance from the great Daniel Auteuil, ``Sade'' covers the same period as Kaufmann's ``Quills'' with more unsettlingly realistic results.\", \"The movie is for fans who can't stop loving anime, and the fanatical excess built into it.\", \"The Ring is worth a look, if you don't demand much more than a few cheap thrills from your Halloween entertainment.\", 'Beautifully observed, miraculously unsentimental comedy-drama.', \"What makes How I Killed My Father compelling, besides its terrific performances, is Fontaine's willingness to wander into the dark areas of parent-child relationships without flinching.\", \"This hastily mounted production exists only to capitalize on Hopkins' inclination to play Hannibal Lecter again, even though Harris has no immediate inclination to provide a fourth book.\", \"Bullock's complete lack of focus and ability quickly derails the film.1\", 'Crush could be the worst film a man has made about women since Valley of the Dolls.', 'Reassuring, retro uplifter.', 'Flaccid drama and exasperatingly slow journey.', 'The Movie will reach far beyond its core demographic.', 'How did it ever get made?', 'I like the new footage and still love the old stuff.', 'It briefly flirts with player masochism, but the point of real interest -- audience sadism -- is evaded completely.', 'Could The Country Bears really be as bad as its trailers?', 'At times, the movie looks genuinely pretty.', 'It depends on how well flatulence gags fit into your holiday concept.', 'The film seems a dead weight.', \"Sparkling, often hilarious romantic jealousy comedy... Attal looks so much like a young Robert DeNiro that it seems the film should instead be called `My Husband Is Travis Bickle'.\", 'I did go back and check out the last 10 minutes, but these were more repulsive than the first 30 or 40 minutes.', \"A lousy movie that's not merely unwatchable, but also unlistenable.\", 'One of the best films of the year with its exploration of the obstacles to happiness faced by five contemporary individuals... a psychological masterpiece.', \"But here's the real damn: It isn't funny, either.\", 'Qutting may be a flawed film, but it is nothing if not sincere.', \"It's a fanboy `what if?'\", '... something appears to have been lost in the translation this time.', 'Like a documentary version of Fight Club, shorn of social insight, intellectual pretension and cinematic interest.', \"Stephen Rea, Aidan Quinn, and Alan Bates play Desmond's legal eagles, and when joined by Brosnan, the sight of this grandiloquent quartet lolling in pretty Irish settings is a pleasant enough thing, `tis.\", \"It's a powerful though flawed movie, guaranteed to put a lump in your throat while reaffirming Washington as possibly the best actor working in movies today.\", 'An intriguing and entertaining introduction to Johnson.', 'Everything was as superficial as the forced New Jersey lowbrow accent Uma had.', 'Despite the holes in the story and the somewhat predictable plot, moments of the movie caused me to jump in my chair...', \"`How many more voyages can this limping but dearly-loved franchise survive?'\", 'If only the story about a multi-million dollar con bothered to include the con.', 'Only a few minutes elapse before the daddy of all slashers arrives, still with the boiler suit and white mask, which look remarkably clean for a guy who has been mass-murdering since 1978 but has never been seen doing laundry.', \"Showtime is one of the hapless victims of the arrogant ``if we put together a wry white man and a chatty black man and give them guns, the movie will be funny'' syndrome.\", 'Whatever heartwarming scene the impressively discreet filmmakers may have expected to record with their mini DV, they show a remarkable ability to document both sides of this emotional car-wreck.', 'A zombie movie in every sense of the word -- mindless, lifeless, meandering, loud, painful, obnoxious.', 'It has fun with the quirks of family life, but it also treats the subject with fondness and respect.', 'Novak manages to capture a cruelly hilarious vein of black comedy in the situation with his cast of non-actors and a gritty, no-budget approach.', 'Since Lee is a sentimentalist, the film is more worshipful than your random E!', 'The movie is almost completely lacking in suspense, surprise and consistent emotional conviction.', 'High Crimes knows the mistakes that bad movies make and is determined not to make them, and maybe that is nobility of a sort.', 'This thing works on no level whatsoever for me.', '... a bland murder-on-campus yawner.', 'In comparison to his earlier films it seems a disappointingly thin slice of lower-class London life; despite the title... amounts to surprisingly little.', 'One of the finest, most humane and important Holocaust movies ever made.', 'Suffers from rambling, repetitive dialogue and the visual drabness endemic to digital video.', 'The twist that ends the movie is the one with the most emotional resonance, but twists are getting irritating, and this is the kind of material where the filmmakers should be very careful about raising eyebrows.', 'Features one of the most affecting depictions of a love affair ever committed to film.', 'It is intensely personal and yet -- unlike Quills -- deftly shows us the temper of the times.', '... a solid, well-formed satire.', 'Deserves high marks for political courage but barely gets by on its artistic merits.', 'Read My Lips is to be viewed and treasured for its extraordinary intelligence and originality as well as its lyrical variations on the game of love.', \"... creates a visceral sense of its characters' lives and conflicted emotions that carries it far above... what could have been a melodramatic, Lifetime Channel-style anthology.\", 'Reeks of rot and hack work from start to finish.', 'Just how these families interact may surprise you.'], 'label': [1, 4, 2, 3, 4, 2, 3, 1, 2, 4, 2, 4, 3, 3, 4, 1, 3, 4, 3, 1, 3, 4, 4, 4, 3, 4, 3, 1, 1, 1, 4, 1, 2, 2, 3, 4, 3, 4, 2, 4, 2, 2, 4, 2, 2, 1, 2, 2, 1, 2, 4, 2, 2, 2, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 1, 2, 2, 4, 2, 2, 2, 1, 2, 2, 1, 4, 4, 4, 4, 4, 2, 4]}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def load_and_format_data(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        data_dict = {'ID': [], 'sentence': [], 'label': []}\n",
    "        for row in reader:\n",
    "            if len(row) >= 3:  # Adjust to handle rows with an extra empty element\n",
    "                # Strip any potential whitespace and ignore empty trailing elements\n",
    "                cleaned_row = [item.strip() for item in row if item.strip()]\n",
    "                if len(cleaned_row) == 3:\n",
    "                    data_dict['ID'].append(cleaned_row[0])\n",
    "                    data_dict['sentence'].append(cleaned_row[1])\n",
    "                    data_dict['label'].append(int(cleaned_row[2]))\n",
    "    return data_dict\n",
    "\n",
    "# Replace 'file_path' with the path to your CSV file\n",
    "file_path = './cleaned_data.csv'\n",
    "data_dictionary = load_and_format_data(file_path)\n",
    "\n",
    "# Display the dictionary to verify\n",
    "print(data_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 128)  # Adjust if different number of BERT features\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 4)  # Assuming 4 classes for sentiment\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class BatchNormNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization for first layer\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)  # Batch normalization for second layer\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)   # Batch normalization for third layer\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))  # Activation after batch norm\n",
    "        x = F.relu(self.bn2(self.fc2(x)))  # Activation after batch norm\n",
    "        x = F.relu(self.bn3(self.fc3(x)))  # Activation after batch norm\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class DeeperNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeeperNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(256, 128)  # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(128, 64)   # Third fully connected layer\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.fc4 = nn.Linear(64, 4)     # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Activation function for first layer\n",
    "        x = F.relu(self.fc2(x))  # Activation function for second layer\n",
    "        x = F.relu(self.fc3(x))  # Activation function for third layer\n",
    "        x = self.dropout(x)      # Apply dropout\n",
    "        x = self.fc4(x)          # Final layer to produce logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.9733\n",
      "Epoch [20/100], Loss: 0.6216\n",
      "Epoch [30/100], Loss: 0.3485\n",
      "Epoch [40/100], Loss: 0.1729\n",
      "Epoch [50/100], Loss: 0.0852\n",
      "Epoch [60/100], Loss: 0.0463\n",
      "Epoch [70/100], Loss: 0.0288\n",
      "Epoch [80/100], Loss: 0.0202\n",
      "Epoch [90/100], Loss: 0.0153\n",
      "Epoch [100/100], Loss: 0.0123\n",
      "Accuracy: 76.47%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # Sample data\n",
    "# data = {\n",
    "#     'sentence': [\n",
    "#         \"One of the best silly horror movies of recent memory, with some real shocks in store for unwary viewers.\",\n",
    "#         \"Jason Patric and Ray Liotta make for one splendidly cast pair.\",\n",
    "#         \"This is more a case of `Sacre bleu!'\",\n",
    "#         \"Presents a good case while failing to provide a reason for us to care beyond the very basic dictums of human decency.\",\n",
    "#         \"Beautifully crafted, engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic, twisty yarn that will keep them guessing.\",\n",
    "#         \"Bread, My Sweet has so many flaws it would be easy for critics to shred it.\",\n",
    "#         \"Ultimately feels empty and unsatisfying, like swallowing a Communion wafer without the wine.\",\n",
    "#         \"Exudes the fizz of a Busby Berkeley musical and the visceral excitement of a sports extravaganza.\"\n",
    "#     ],\n",
    "#     'label': [1, 4, 2, 3, 4, 2, 3, 1]\n",
    "# }\n",
    "\n",
    "df = pd.DataFrame(data_dictionary)\n",
    "\n",
    "# Adjust labels to be zero-indexed if not already\n",
    "df['label'] = df['label'] - 1\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization and encoding\n",
    "tokens = tokenizer(df['sentence'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state[:,0,:].numpy()  # Taking the output of the first token (CLS token)\n",
    "\n",
    "# Convert embeddings and labels to DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "df = pd.concat([df, embeddings_df], axis=1)\n",
    "df.drop(['sentence', 'ID'], axis=1, inplace=True)  # Drop 'sentence' and 'ID' if not needed for training\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values).float()\n",
    "X_test = torch.tensor(X_test.values).float()\n",
    "y_train = torch.tensor(y_train.values).long()\n",
    "y_test = torch.tensor(y_test.values).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model1 = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Adjust number of epochs as needed\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model1(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model1(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2877\n",
      "Epoch [20/100], Loss: 0.1555\n",
      "Epoch [30/100], Loss: 0.0939\n",
      "Epoch [40/100], Loss: 0.0615\n",
      "Epoch [50/100], Loss: 0.0435\n",
      "Epoch [60/100], Loss: 0.0326\n",
      "Epoch [70/100], Loss: 0.0257\n",
      "Epoch [80/100], Loss: 0.0209\n",
      "Epoch [90/100], Loss: 0.0175\n",
      "Epoch [100/100], Loss: 0.0149\n",
      "Accuracy: 70.59%\n"
     ]
    }
   ],
   "source": [
    "model2 = BatchNormNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Adjust number of epochs as needed\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model2(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model2(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3061\n",
      "Epoch [20/100], Loss: 0.1670\n",
      "Epoch [30/100], Loss: 0.1011\n",
      "Epoch [40/100], Loss: 0.0663\n",
      "Epoch [50/100], Loss: 0.0467\n",
      "Epoch [60/100], Loss: 0.0351\n",
      "Epoch [70/100], Loss: 0.0276\n",
      "Epoch [80/100], Loss: 0.0226\n",
      "Epoch [90/100], Loss: 0.0189\n",
      "Epoch [100/100], Loss: 0.0161\n",
      "Accuracy: 64.71%\n"
     ]
    }
   ],
   "source": [
    "model3 = BatchNormNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Adjust number of epochs as needed\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model3(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model3.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model3(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
